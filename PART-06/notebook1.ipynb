{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HF_TOKEN = os.environ.get('HF_TOKEN')\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e15e9e5b0484e368b6fc45cb684fa19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\parth\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3ed372af0a4574823a173fdc37dbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031200022051443690dd559b91d18472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745ce81ae3ee4dedb5fe156fead4b718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint\n",
    "\n",
    "model = HuggingFaceEndpoint(\n",
    "    repo_id='mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    task='text-generation'\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat.invoke(\"what is machine learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "data_path = 'data'\n",
    "loader = DirectoryLoader(\n",
    "    data_path,\n",
    "    glob='*.pdf',\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.72s/it]\n"
     ]
    }
   ],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\parth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = embeddings.embed_query(\"what is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "index_name = 'ragfusion'\n",
    "PINECONE_API_KEY = \"pcsk_4EoYj2_UAJvkYfhDcU3fSxaLWTmD4Vj96XAUmLgSa91bmxWF4ja6MZPdivCcps61uzpmJX\"\n",
    "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
    "vectorstore = Pinecone.from_documents(chunks, embedding=embeddings, index_name=index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievar = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\Efficiency-Driven_Custom_Chatbot_Development_Unlea.pdf'}, page_content='i) Expanding the Knowledge Horizon: Retrieval-Augmented Generation.\\n\\nRAG operates under a two-stage paradigm that extends the capabilities established through fine-\\n\\ntuning. Here is how RAG builds upon the foundation laid by fine-tuning.\\n\\nCMC, 2024'),\n",
       " Document(metadata={'source': 'data\\\\Efficiency-Driven_Custom_Chatbot_Development_Unlea.pdf'}, page_content='CMC, 2024\\n\\nFigure 3: RAG sequence diagram\\n\\n3.6 ChromaDB as a Vector Database'),\n",
       " Document(metadata={'source': 'data\\\\Efficiency-Driven_Custom_Chatbot_Development_Unlea.pdf'}, page_content='3.5 Executing RAG with LangChain\\n\\nLangChain, a flexible library for building NLP pipelines, works with the consistent reconciliation\\n\\nof RAG inside our fine-tuned LLM structure. We can leverage Langchain’s abilities to:'),\n",
       " Document(metadata={'source': 'data\\\\Efficiency-Driven_Custom_Chatbot_Development_Unlea.pdf'}, page_content='accurate data. RAG further improves precision by permitting the chatbot to get to and coordinate modern data from the outside information base. This joined methodology guarantees that the chatbot’s reactions are grounded in dependable and exact data, a funda- mental viewpoint for spaces like medical services.')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrievar.invoke(\"what is RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "If provided context doesn't contain answer then don't makeup any answer. if you don't know the answer then just say I don't know.\n",
    "Provide only helpful answers\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\nIf provided context doesn't contain answer then don't makeup any answer. if you don't know the answer then just say I don't know.\\nProvide only helpful answers\\n\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatHuggingFace(llm=HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', stop_sequences=[], server_kwargs={}, model_kwargs={}, model='mistralai/Mistral-7B-Instruct-v0.2', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, task='text-generation'), tokenizer=LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "), model_id='mistralai/Mistral-7B-Instruct-v0.2')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    chain_type='stuff',\n",
    "    retriever=retrievar,\n",
    "    llm=chat \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" RAG stands for Retrieval-Augmented Generation.It is a two-stage paradigm that expands the capabilities established through fine-tuning of a language model. RAG operates by retrieving information from a vector database like ChromaDB and coordinating it with the model's response to generate answers that are more accurate and grounded in dependable and exact data. This helps ensure that the chatbot's reactions are based on reliable information.\""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"what is RAG\")['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain(\"tell me the data ingestion process in Rag Pipeline\")['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " In the RAG pipeline, data is likely ingested through LangChain, a flexible library used for building NLP pipelines. LangChain's abilities allow for the consistent reconciliation of RAG within the fine-tuned Language Model (LLM) structure. This means that RAG builds upon the capabilities established by fine-tuning by accessing modern, reliable data from the outside information base to improve precision in its responses. The sequence diagram in Figure 3 illustrates this process."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model='deepseek-r1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"tell me the best sex position?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so I'm trying to figure out what the best sex position is. I know that people have different preferences and there's a lot of advice online about this. But honestly, I don't really know where to start. Let me think through this.\n",
       "\n",
       "First off, I guess it depends on what makes two people comfortable and happy. So maybe I should consider factors like communication, comfort during the act, and after. I remember reading somewhere that trust is important in any intimate setting, so maybe open communication between partners is key here.\n",
       "\n",
       "Then there's the physical aspect of each position. Some positions might be more comfortable for certain body types or sizes. For example, the horse kick or cobra position sounds intense but maybe not everyone can handle it without pain. On the other hand, the cat spread is supposed to relieve tightness and relax muscles in the pelvis.\n",
       "\n",
       "I've heard that some people prefer the pressure point at the G-spot because it's said to cause intense orgasm. But I also know that not everyone experiences the same sensations, so this might be subjective. Maybe it's best for partners to discuss which method works best together without feeling pressured.\n",
       "\n",
       "Another thing is preparation. Taking time before the actual act makes sense—like emotional intimacy or foreplay can set a positive tone and make the experience more enjoyable overall. But what exactly counts as preparation? Maybe it's just cuddling, talking, or playing with each other's bodies in a relaxed way.\n",
       "\n",
       "Post-coital communication is something I've heard about but don't really understand. How do people typically talk after having sex? Do they ask if it was enjoyable, check for any physical discomfort, or just have an open conversation to make sure everything went well?\n",
       "\n",
       "I'm also thinking about the psychological aspect of intimacy. It's not just about the act itself but also building trust and emotional connection before engaging in sexual positions. Without trust, maybe some positions might be too intense or could lead to negative feelings.\n",
       "\n",
       "But then again, are there any risks involved with certain positions? For example, the horse kick involves a lot of movement, which could potentially cause injury if not done correctly. So it's probably important for partners to know each other well before trying something like that to minimize risk.\n",
       "\n",
       "I wonder if there's an ideal position that works for everyone or if it's really about personal preference. Maybe some positions are universally liked because they offer a certain level of comfort and pleasure, but others might be better suited for specific body types or relationships.\n",
       "\n",
       "It's also possible that the best position changes over time as partners' preferences and relationship dynamics evolve. So maybe there isn't one single \"best\" position but rather something that both partners agree on through open communication and mutual respect.\n",
       "\n",
       "In summary, I think figuring out the best sex position involves considering communication, comfort, preparation, and post-coital interaction. It's all about finding what works for each couple individually without forcing a particular method on someone else.\n",
       "</think>\n",
       "\n",
       "The \"best\" sex position is highly subjective and depends on individual preferences, comfort levels, and relationship dynamics. Here are some key considerations to help determine the best position for any couple:\n",
       "\n",
       "1. **Communication**: Open and honest communication is crucial. Each partner should feel comfortable discussing their preferences and ensuring that both are satisfied.\n",
       "\n",
       "2. **Comfort and Safety**: Choose a position that suits your bodies and avoids pain or injury. Research each position's potential risks, such as the horse kick's physical demands, and ensure you're prepared with proper knowledge and care.\n",
       "\n",
       "3. **Preparation**: Invest time in emotional intimacy through activities like cuddling, talking, or playful body contact to set a positive tone before engaging in a new position.\n",
       "\n",
       "4. **Post-coital Interaction**: After sex, having an open conversation about the experience can help ensure both partners are satisfied and build trust in your relationship.\n",
       "\n",
       "5. **Trust and Connection**: Building trust is essential for any intimate activity. Ensure you're comfortable with each other's openness regarding sexual preferences and boundaries.\n",
       "\n",
       "6. **Psychological Intimacy**: Focus on building emotional connection before engaging in physical acts, as this can enhance overall satisfaction.\n",
       "\n",
       "There isn't a universal \"best\" position; it's about finding what works for both partners through mutual respect and open dialogue. Consider discussing various positions with your partner to determine which one suits you best without feeling pressured or uncomfortable."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
